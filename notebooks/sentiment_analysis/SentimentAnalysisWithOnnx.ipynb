{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2170ea0-d6f1-4d71-947e-f7f459da466a",
   "metadata": {},
   "source": [
    "# One time Setup\n",
    "- uv sync --group dev\n",
    "- uv add onnxruntime transformers torch\n",
    "- uv pip list\n",
    "- uv run python -c \"import torch; import onnx; import onnxruntime; import transformers; print('All packages installed successfully with uv!')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e92657b-a22f-4e34-8ffc-55d95885c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed successfully with uv!\n"
     ]
    }
   ],
   "source": [
    "!uv run python -c \"import torch; import onnx; import onnxruntime; import transformers; print('All packages installed successfully with uv!')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de070e85-baa0-4528-8e7c-e523741c6f3d",
   "metadata": {},
   "source": [
    "# 1: Create/Load/Infer Onnx Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa70231-381f-4e2f-8dc9-34b2b5f48921",
   "metadata": {},
   "source": [
    "### 1.1 Load Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b2e038-b47a-4de1-9659-50729de7305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: cardiffnlp/twitter-roberta-base-sentiment-latest\n"
     ]
    }
   ],
   "source": [
    "# Command 1: Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Command 2: Set model name\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "print(f\"Model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a71b72-13db-4dfd-8ffc-5d4a8d74fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model loaded\n",
      "Dummy input created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Command 3: Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "# Command 4: Load PyTorch model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "print(\"PyTorch model loaded\")\n",
    "\n",
    "# Command 5: Create dummy input\n",
    "dummy_input = tokenizer(\n",
    "    \"This is a test sentence.\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "print(\"Dummy input created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72803186-e5f4-4c71-989a-ef49e3b1b061",
   "metadata": {},
   "source": [
    "### 1.2 Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf07acb-e948-4153-957a-b1c6a6a0573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /Users/chang/.cache/huggingface/hub/onnx_models/cardiffnlp_twitter-roberta-base-sentiment-latest\n"
     ]
    }
   ],
   "source": [
    "# Command 6: Create output directory\n",
    "output_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"onnx_models\" / model_name.replace(\"/\", \"_\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "onnx_path = output_dir / \"model.onnx\"\n",
    "print(f\"Output directory: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e8fb68-6e81-46a7-8f70-de9ba1698f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd78485-24df-465c-acab-1c6192715dfe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36a1e2-7982-4fd1-9418-660a68c82d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e7cb8-57ce-4c59-856c-b1f0991445ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Command 7: Convert to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "    str(onnx_path),\n",
    "    export_params=True,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "        'logits': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "print(f\"ONNX model saved to: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa9653-f4e2-4ba6-93cd-f64a63ce9339",
   "metadata": {},
   "source": [
    "### 1.3 SKIP TO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ce26a5-7ca3-4fda-930d-a8bc312f666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded\n",
      "-rw-r--r--@ 1 chang  staff  498864302 Aug 18 14:49 /Users/chang/.cache/huggingface/hub/onnx_models/cardiffnlp_twitter-roberta-base-sentiment-latest/model.onnx\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "onnx_path='/Users/chang/.cache/huggingface/hub/onnx_models/cardiffnlp_twitter-roberta-base-sentiment-latest/model.onnx'\n",
    "!ls -l {onnx_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b13ed-0c33-44b6-ace3-c43241cc0ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "839b48b9-503d-4ec9-a73c-2d89801f1b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded\n"
     ]
    }
   ],
   "source": [
    "# Command 8: Load ONNX model\n",
    "# https://onnxruntime.ai/docs/tutorials/\n",
    "onnx_session = ort.InferenceSession(str(onnx_path))\n",
    "\n",
    "#sess_opt = SessionOptions()\n",
    "#sess_opt.log_severity_level = 0 // Verbose\n",
    "\n",
    "#onnx_session = ort.InferenceSession(\n",
    "#                model_path,\n",
    "#                sess_options=sess_opt, #https://onnxruntime.ai/docs/api/python/api_summary.html#sessionoptions\n",
    "#                providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] #https://onnxruntime.ai/docs/execution-providers/\n",
    "#            )\n",
    "\n",
    "print(\"ONNX model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd5a4f8-79e7-42c6-9516-600959bee8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.037826  , -1.2209561 ,  3.316388  ],\n",
       "        [ 2.1412935 , -0.4224534 , -1.9739852 ],\n",
       "        [-1.728252  ,  0.5253647 ,  0.74663556]], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Command 9: Test the model\n",
    "test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "\n",
    "# Command 10: Tokenize test texts\n",
    "tokenized = tokenizer(\n",
    "    test_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"np\"\n",
    ")\n",
    "\n",
    "# Command 11: Run inference\n",
    "outputs = onnx_session.run(\n",
    "    None,\n",
    "    {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask']\n",
    "    }\n",
    ")\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb5c1ddb-d68c-40f8-a409-045533d4693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities=[[0.00465634 0.01053917 0.9848045 ]\n",
      " [0.914631   0.07044088 0.01492805]\n",
      " [0.04463791 0.42504716 0.5303149 ]]\n",
      "\n",
      "Test Results:\n",
      "Text: I love this product!\n",
      "Prediction: positive\n",
      "Probabilities: [0.00465634 0.01053917 0.9848045 ]\n",
      "----------------------------------------\n",
      "Text: This is terrible!\n",
      "Prediction: negative\n",
      "Probabilities: [0.914631   0.07044088 0.01492805]\n",
      "----------------------------------------\n",
      "Text: It's okay.\n",
      "Prediction: positive\n",
      "Probabilities: [0.04463791 0.42504716 0.5303149 ]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process results\n",
    "logits = outputs[0]\n",
    "probabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "print(f'probabilities={probabilities}\\n')\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "print(\"Test Results:\")\n",
    "for i, text in enumerate(test_texts):\n",
    "    pred_label = labels[np.argmax(probabilities[i])]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {pred_label}\")\n",
    "    print(f\"Probabilities: {probabilities[i]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf1e04-5755-47ae-aea8-a270b3c81540",
   "metadata": {},
   "source": [
    "### 1.4 API Summary\n",
    "- https://onnxruntime.ai/docs/api/python/api_summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34730261-bcfc-4ac9-832e-f01cc2ac4603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a76a36-a664-4729-9d1d-8e2076decce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55c9ba72-adb0-44f3-a60a-7210d37b47a3",
   "metadata": {},
   "source": [
    "# 2 Use Onnx_memory_tracking module\n",
    "- code is from example_usage_factored.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f56d6a-8c9a-4185-82f2-0df0f185eb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from onnx_memory_tracking import (\n",
    "    track_with_psutil,\n",
    "    track_with_tracemalloc,\n",
    "    track_with_onnx_providers,\n",
    "    track_disk_io,\n",
    "    track_with_system_monitor,\n",
    "    get_onnx_model_info,\n",
    "    create_onnx_session_with_profiling,\n",
    "    run_comprehensive_tracking,\n",
    "    track_model_unloading,\n",
    "    track_memory_leaks_during_unloading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e8ce0-0540-40c8-a3f7-e942e259d6ba",
   "metadata": {},
   "source": [
    "## 2.1 Main Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "844b951e-e57c-49ee-aaa7-a30dfd1227f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1: Basic Usage ===\n",
      "=== Method 1: psutil Memory Tracking ===\n",
      "Execution time: 0.0214 seconds\n",
      "Memory used: 0.17 MB\n",
      "Peak memory: 1194.94 MB\n",
      "Virtual memory: 405377.36 MB\n",
      "Basic tracking completed. Memory used: 0.17 MB\n"
     ]
    }
   ],
   "source": [
    "# WARNING: tracker is not thread safe, make it a singleton\n",
    "# tracker = ONNXMemoryTracker() \n",
    "# tracker.start_tracking()\n",
    "# .....\n",
    "# metrics = tracker.end_tracking()\n",
    "\n",
    "def example_basic_usage(onnx_path: Path):\n",
    "    \"\"\"Example 1: Basic usage with your own ONNX model and tokenizer.\"\"\"\n",
    "    print(\"=== Example 1: Basic Usage ===\")\n",
    "    \n",
    "    # Load your model and tokenizer\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    \n",
    "    if not onnx_path.exists():\n",
    "        print(f\"ONNX model not found at {onnx_path}\")\n",
    "        return\n",
    "    \n",
    "    # Create your ONNX session\n",
    "    onnx_session = ort.InferenceSession(str(onnx_path))\n",
    "    \n",
    "    # Your test data\n",
    "    test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "    \n",
    "    # Run basic memory tracking\n",
    "    metrics = track_with_psutil(onnx_session, tokenizer, test_texts)\n",
    "    print(f\"Basic tracking completed. Memory used: {metrics['memory_used'] / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\n",
    "example_basic_usage(Path(onnx_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98d5c1b2-6fee-43df-88eb-be3dc1cc31fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7c69c00-b99d-4c84-aa60-3e6ceb3a7e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 2: Pre-processed Data ===\n",
      "=== Method 1: psutil Memory Tracking ===\n",
      "Execution time: 0.0210 seconds\n",
      "Memory used: 0.50 MB\n",
      "Peak memory: 1090.75 MB\n",
      "Virtual memory: 405380.34 MB\n",
      "Tracking with pre-processed data completed. Memory used: 0.50 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def example_with_preprocessed_data(onnx_path):\n",
    "    \"\"\"Example 2: Using pre-processed input data.\"\"\"\n",
    "    print(\"\\n=== Example 2: Pre-processed Data ===\")\n",
    "    \n",
    "    # Load your model and tokenizer\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    if not onnx_path.exists():\n",
    "        print(f\"ONNX model not found at {onnx_path}\")\n",
    "        return\n",
    "    \n",
    "    # Create your ONNX session\n",
    "    onnx_session = ort.InferenceSession(str(onnx_path))\n",
    "    \n",
    "    # Pre-process your data\n",
    "    test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "    tokenized = tokenizer(\n",
    "        test_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    \n",
    "    input_data = {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask']\n",
    "    }\n",
    "    \n",
    "    # Run tracking with pre-processed data\n",
    "    metrics = track_with_psutil(onnx_session, tokenizer, input_data=input_data)\n",
    "    print(f\"Tracking with pre-processed data completed. Memory used: {metrics['memory_used'] / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "example_with_preprocessed_data(Path(onnx_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7bf9104-e3ee-4540-94da-a5f1688ff62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 3: ONNX Profiling ===\n",
      "\n",
      "=== Method 3: ONNX Runtime Provider Tracking ===\n",
      "Profiling completed. Profile file: my_profile_2025-08-18_16-11-37.json\n",
      "Profiling completed. Profile file: my_profile_2025-08-18_16-11-37.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def example_with_profiling(onnx_path):\n",
    "    \"\"\"Example 3: Using ONNX profiling.\"\"\"\n",
    "    print(\"\\n=== Example 3: ONNX Profiling ===\")\n",
    "    \n",
    "    # Load your model and tokenizer\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Find your ONNX model\n",
    "    onnx_path = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"onnx_models\" / model_name.replace(\"/\", \"_\") / \"model.onnx\"\n",
    "    \n",
    "    if not onnx_path.exists():\n",
    "        print(f\"ONNX model not found at {onnx_path}\")\n",
    "        return\n",
    "    \n",
    "    # Create ONNX session with profiling enabled\n",
    "    profiling_session = create_onnx_session_with_profiling(onnx_path, \"my_profile\")\n",
    "    \n",
    "    # Your test data\n",
    "    test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "    \n",
    "    # Run profiling\n",
    "    profile_results = track_with_onnx_providers(profiling_session, tokenizer, test_texts, num_runs=10)\n",
    "    print(f\"Profiling completed. Profile file: {profile_results['profile_file']}\")\n",
    "\n",
    "example_with_profiling(Path(onnx_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4f0c2a2-f04a-42e4-a1d3-9c5660430108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 4: Comprehensive Tracking ===\n",
      "Running basic memory tracking...\n",
      "=== Method 1: psutil Memory Tracking ===\n",
      "Execution time: 0.0199 seconds\n",
      "Memory used: 0.11 MB\n",
      "Peak memory: 1183.84 MB\n",
      "Virtual memory: 405385.34 MB\n",
      "\n",
      "Running tracemalloc analysis...\n",
      "\n",
      "=== Method 2: tracemalloc Memory Tracking ===\n",
      "Top 10 memory differences:\n",
      "+3 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 553', '    encodings = self._tokenizer.encode_batch(']\n",
      "+4 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 756', '    return np.asarray(value, dtype=dtype)']\n",
      "+6 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 273', '    return self._sess.run(output_names, input_feed, run_options)']\n",
      "+2 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/.pyenv/versions/3.11.11/lib/python3.11/tracemalloc.py\", line 560', '    return Snapshot(traces, traceback_limit)']\n",
      "+2 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/.pyenv/versions/3.11.11/lib/python3.11/tracemalloc.py\", line 423', '    self.traces = _Traces(traces)']\n",
      "+2 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 601', '    return BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type=return_tensors)']\n",
      "+3 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 344', '    encoding_dict[\"attention_mask\"].append(e.attention_mask)']\n",
      "+3 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 339', '    encoding_dict[\"input_ids\"].append(e.ids)']\n",
      "+1 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 750', '    def as_tensor(value, dtype=None):']\n",
      "+2 blocks: 0.0 MB\n",
      "  ['  File \"/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 1081', '    key_without_id = key[:-3] if not key.endswith(\"_ids\") else key[:-4]']\n",
      "\n",
      "Current memory usage: 0.0 MB\n",
      "Peak memory usage: 0.0 MB\n",
      "\n",
      "Running disk I/O tracking...\n",
      "\n",
      "=== Method 4: Disk I/O Tracking ===\n",
      "Disk reads: 0 bytes\n",
      "Disk writes: 0 bytes\n",
      "Disk read count: 0\n",
      "Disk write count: 0\n",
      "\n",
      "Getting model information...\n",
      "\n",
      "=== ONNX Model Information ===\n",
      "Input information:\n",
      "  Name: input_ids\n",
      "  Shape: ['batch_size', 'sequence']\n",
      "  Type: tensor(int64)\n",
      "  Name: attention_mask\n",
      "  Shape: ['batch_size', 'sequence']\n",
      "  Type: tensor(int64)\n",
      "Output information:\n",
      "  Name: logits\n",
      "  Shape: ['batch_size', 3]\n",
      "  Type: tensor(float)\n",
      "Available providers: ['CPUExecutionProvider']\n",
      "\n",
      "Comprehensive tracking results:\n",
      "  psutil: dict\n",
      "  tracemalloc: dict\n",
      "  disk_io: dict\n",
      "  model_info: dict\n"
     ]
    }
   ],
   "source": [
    "def example_comprehensive_tracking(onnx_path):\n",
    "    \"\"\"Example 4: Comprehensive tracking with all methods.\"\"\"\n",
    "    print(\"\\n=== Example 4: Comprehensive Tracking ===\")\n",
    "    \n",
    "    # Load your model and tokenizer\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    # Create your ONNX session\n",
    "    onnx_session = ort.InferenceSession(str(onnx_path))\n",
    "    \n",
    "    # Your test data\n",
    "    test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "    \n",
    "    # Run comprehensive tracking\n",
    "    results = run_comprehensive_tracking(\n",
    "        onnx_session=onnx_session,\n",
    "        tokenizer=tokenizer,\n",
    "        test_texts=test_texts,\n",
    "        enable_debug=True,      # Enable tracemalloc\n",
    "        enable_optimization=False,  # Disable ONNX profiling (would need separate session)\n",
    "        enable_disk_io=True,    # Enable disk I/O tracking\n",
    "        enable_real_time=False  # Disable real-time monitoring\n",
    "    )\n",
    "    \n",
    "    print(\"\\nComprehensive tracking results:\")\n",
    "    for method, result in results.items():\n",
    "        print(f\"  {method}: {type(result).__name__}\")\n",
    "\n",
    "example_comprehensive_tracking(Path(onnx_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb8a73-fd7c-49a5-a5c5-139ea5db5d92",
   "metadata": {},
   "source": [
    "## 2.2 Model Unloading\n",
    "= Model unloading: see model_example_unloading.py\n",
    "- Metrics\n",
    "    * Memory freed (negative memory_used)\n",
    "    * Final memory footprint (memory_current)\n",
    "    * Relative impact (memory_percent_change)\n",
    "    * Memory leaks (run multiple cycles)\n",
    "    * Unloading efficiency (compare with loading)\n",
    "- Metric\t|  What to Look For\t|  Good Sign\t|  Bad Sign\n",
    "    * memory_used |\tShould be negative\t| -50 MB\t| +10 MB\n",
    "    * memory_current|\tShould decrease\t| 100 MB → 50 MB\t| 100 MB → 110 MB\n",
    "    * memory_percent_change |\tShould be negative\t| -0.5% |\t+0.1%\n",
    "    * unloading_successful |\tShould be True\t| True\t| False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c28583f-c2eb-44a8-a67b-5c600870b726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chang/Documents/dev/git/ml/coachKata/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from onnx_memory_tracking import (\n",
    "    track_with_psutil,\n",
    "    track_model_unloading,\n",
    "    track_memory_leaks_during_unloading,\n",
    "    unload_model_with_tracking,\n",
    "\n",
    "    track_onnx_runtime_memory_release,\n",
    "    demonstrate_actual_onnx_memory_release\n",
    ")\n",
    "\n",
    "onnx_path='/Users/chang/.cache/huggingface/hub/onnx_models/cardiffnlp_twitter-roberta-base-sentiment-latest/model.onnx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9504628-c8ec-48b1-b015-fe3bd93c9c96",
   "metadata": {},
   "source": [
    "##### Basic  - INaccurate; only reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2629c50-14db-4505-b232-ce10cffedab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Testing basic unloading tracking...\n",
      "=== Model Unloading Memory Tracking ===\n",
      "Final inference completed\n",
      "Unloading model...\n",
      "Unloading time: 0.0413 seconds\n",
      "Memory before unload: 1329.09 MB\n",
      "Memory after unload: 1329.09 MB\n",
      "Memory freed: 0.00 MB\n",
      "Memory freed percentage: 0.00%\n",
      "✅ Model unloading successful - memory freed or stable\n",
      "✅ Basic unloading tracking completed\n",
      "   Success: True\n",
      "   Memory freed: 0.00 MB\n",
      "✅ All expected keys present\n",
      "\n",
      "2. Testing actual unloading tracking...\n",
      "=== Actual Model Unloading with Tracking ===\n",
      "Final inference completed\n",
      "Actually unloading model...\n",
      "Unloading time: 0.0375 seconds\n",
      "Memory before unload: 1716.77 MB\n",
      "Memory after unload: 1716.92 MB\n",
      "Memory freed: -0.16 MB\n",
      "Memory freed percentage: -0.00%\n",
      "✅ Model unloading successful - memory freed or stable\n",
      "✅ Actual unloading tracking completed\n",
      "   Success: True\n",
      "   Memory freed: -0.16 MB\n",
      "\n",
      "✅ All tests passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def example_basic_unloading_v2(onnx_path=onnx_path):\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Create ONNX session\n",
    "    onnx_session = ort.InferenceSession(str(onnx_path))\n",
    "    \n",
    "    # Test texts\n",
    "    test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "    \n",
    "    print(\"1. Testing basic unloading tracking...\")\n",
    "    try:\n",
    "        unloading_metrics = track_model_unloading(\n",
    "            onnx_session=onnx_session,\n",
    "            tokenizer=tokenizer,\n",
    "            test_texts=test_texts\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Basic unloading tracking completed\")\n",
    "        print(f\"   Success: {unloading_metrics['unloading_successful']}\")\n",
    "        print(f\"   Memory freed: {unloading_metrics['memory_freed_mb']:.2f} MB\")\n",
    "        \n",
    "        # Check if the function returns expected keys\n",
    "        expected_keys = [\n",
    "            'unloading_successful', 'memory_freed_bytes', 'memory_freed_mb',\n",
    "            'memory_freed_percent', 'memory_before_unload', 'memory_after_unload',\n",
    "            'memory_before_percent', 'memory_after_percent', 'unloading_time'\n",
    "        ]\n",
    "        \n",
    "        missing_keys = [key for key in expected_keys if key not in unloading_metrics]\n",
    "        if missing_keys:\n",
    "            print(f\"❌ Missing keys: {missing_keys}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"✅ All expected keys present\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Basic unloading tracking failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\\n2. Testing actual unloading tracking...\")\n",
    "    try:\n",
    "        # Create new session and tokenizer for actual unloading test\n",
    "        tokenizer2 = AutoTokenizer.from_pretrained(model_name)\n",
    "        onnx_session2 = ort.InferenceSession(str(onnx_path))\n",
    "        \n",
    "        unloading_metrics2 = unload_model_with_tracking(\n",
    "            onnx_session=onnx_session2,\n",
    "            tokenizer=tokenizer2,\n",
    "            test_texts=test_texts\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Actual unloading tracking completed\")\n",
    "        print(f\"   Success: {unloading_metrics2['unloading_successful']}\")\n",
    "        print(f\"   Memory freed: {unloading_metrics2['memory_freed_mb']:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Actual unloading tracking failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\\n✅ All tests passed!\")\n",
    "    return True\n",
    "\n",
    "example_basic_unloading_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f8e5c-8de7-4e71-8200-e8719f64b28e",
   "metadata": {},
   "source": [
    "##### MOre accurate: System + Process memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5b2c68-a861-4652-a111-ff20b88abf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ONNX Runtime Memory Release Tracking ===\n",
      "Initial system memory: 4716.66 MB available\n",
      "Initial process memory: 1425.16 MB RSS\n",
      "Final inference completed\n",
      "Releasing ONNX Runtime memory...\n",
      "  - Session providers: ['CPUExecutionProvider']\n",
      "  - Session inputs: ['input_ids', 'attention_mask']\n",
      "  - Session outputs: ['logits']\n",
      "  - Destroying ONNX session...\n",
      "\n",
      "--- Memory Release Results ---\n",
      "Unloading time: 0.0745 seconds\n",
      "Process memory freed: -0.16 MB\n",
      "System memory freed: -5.58 MB\n",
      "Process memory freed: -0.97 MB\n",
      "Memory freed percentage: -0.00%\n",
      "✅ ONNX Runtime memory release successful\n",
      "{'unloading_successful': True, 'memory_freed_bytes': -163840, 'memory_freed_mb': -0.15625, 'memory_freed_percent': -0.00095367431640625, 'memory_before_unload': 1495236608, 'memory_after_unload': 1495400448, 'memory_before_percent': 8.703422546386719, 'memory_after_percent': 8.704376220703125, 'unloading_time': 0.07452392578125, 'system_memory_freed_mb': -5.578125, 'process_memory_freed_mb': -0.96875, 'session_providers': ['CPUExecutionProvider'], 'session_inputs': ['input_ids', 'attention_mask'], 'session_outputs': ['logits']}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "onnx_session = ort.InferenceSession(str(onnx_path))\n",
    "test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "\n",
    "# More similear to my usecase\n",
    "release_metrics = track_onnx_runtime_memory_release(\n",
    "    onnx_session=onnx_session,\n",
    "    tokenizer=tokenizer,\n",
    "    test_texts=test_texts\n",
    ")\n",
    "print(release_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0dec0-e275-4ac4-92bc-3ac48ba5881a",
   "metadata": {},
   "source": [
    "##### More accurate, but non suited my use case: Process separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4da688-0bfe-46b8-8ff0-68c2a0737181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demonstrating Actual ONNX Memory Release ===\n",
      "Initial system memory: 4649.78 MB available\n",
      "Initial process memory: 1515.89 MB RSS\n",
      "Running model in separate process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and unloaded successfully in separate process\n",
      "Process output: Inference completed in subprocess\n",
      "Session ready for destruction\n",
      "\n",
      "--- Memory Release Demonstration Results ---\n",
      "System memory change: -9.06 MB\n",
      "Process memory change: +458.12 MB\n",
      "Final system memory: 4640.72 MB available\n",
      "Final process memory: 1057.77 MB RSS\n",
      "✅ Actual ONNX memory release demonstrated!\n",
      "   This shows that ONNX Runtime does release memory when sessions are destroyed\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "onnx_session = ort.InferenceSession(str(onnx_path))\n",
    "test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "\n",
    "demonstration_results = demonstrate_actual_onnx_memory_release(\n",
    "    onnx_path=onnx_path,\n",
    "    model_name=model_name,\n",
    "    test_texts=test_texts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c7a64-10ab-4a2f-8852-21ba47aeea8e",
   "metadata": {},
   "source": [
    "##### A more releastic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b372c8-992f-4fc1-83ca-fe07ab0ef732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CardiffNLP Model Size Smoke Check ===\n",
      "ONNX file size: 475.75 MB\n",
      "ONNX session providers: ['CPUExecutionProvider']\n",
      "Inputs: ['input_ids', 'attention_mask']\n",
      "Outputs: ['logits']\n",
      "\n",
      "=== Memory Analysis ===\n",
      "ONNX file size: 475.75 MB\n",
      "Your observed memory release: 41 MB average\n",
      "Memory release ratio: 0.09x\n",
      "\n",
      "=== Expected Ranges ===\n",
      "Minimum expected (file size): 475.75 MB\n",
      "Typical expected (1.5-3x file size): 713.63 - 1427.26 MB\n",
      "Maximum expected (5x file size): 2378.77 MB\n",
      "\n",
      "=== Assessment ===\n",
      "❌ 41MB is LESS than file size (475.75MB) - This suggests incomplete unloading\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def smoke_check_model_size():\n",
    "    \"\"\"Smoke check to verify if 41MB memory release makes sense.\"\"\"\n",
    "    \n",
    "    # CardiffNLP model path\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    onnx_path = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"onnx_models\" / model_name.replace(\"/\", \"_\") / \"model.onnx\"\n",
    "    \n",
    "    print(\"=== CardiffNLP Model Size Smoke Check ===\")\n",
    "    \n",
    "    # 1. Check ONNX file size\n",
    "    if onnx_path.exists():\n",
    "        file_size_mb = onnx_path.stat().st_size / 1024 / 1024\n",
    "        print(f\"ONNX file size: {file_size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"ONNX file not found at {onnx_path}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Check HuggingFace model size\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Get model size from HuggingFace cache\n",
    "    cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "    model_cache_dir = cache_dir / model_name.replace(\"/\", \"_\")\n",
    "    \n",
    "    if model_cache_dir.exists():\n",
    "        total_size = sum(f.stat().st_size for f in model_cache_dir.rglob('*') if f.is_file())\n",
    "        model_size_mb = total_size / 1024 / 1024\n",
    "        print(f\"HuggingFace model cache size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    # 3. Load ONNX session and check memory usage\n",
    "    session = ort.InferenceSession(str(onnx_path))\n",
    "    \n",
    "    # Get session info\n",
    "    inputs = session.get_inputs()\n",
    "    outputs = session.get_outputs()\n",
    "    providers = session.get_providers()\n",
    "    \n",
    "    print(f\"ONNX session providers: {providers}\")\n",
    "    print(f\"Inputs: {[input.name for input in inputs]}\")\n",
    "    print(f\"Outputs: {[output.name for output in outputs]}\")\n",
    "    \n",
    "    # 4. Memory analysis\n",
    "    print(f\"\\n=== Memory Analysis ===\")\n",
    "    print(f\"ONNX file size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"Your observed memory release: 41 MB average\")\n",
    "    print(f\"Memory release ratio: {41/file_size_mb:.2f}x\")\n",
    "    \n",
    "    # 5. Expected ranges\n",
    "    print(f\"\\n=== Expected Ranges ===\")\n",
    "    print(f\"Minimum expected (file size): {file_size_mb:.2f} MB\")\n",
    "    print(f\"Typical expected (1.5-3x file size): {file_size_mb*1.5:.2f} - {file_size_mb*3:.2f} MB\")\n",
    "    print(f\"Maximum expected (5x file size): {file_size_mb*5:.2f} MB\")\n",
    "    \n",
    "    # 6. Assessment\n",
    "    print(f\"\\n=== Assessment ===\")\n",
    "    if 41 < file_size_mb:\n",
    "        print(f\"❌ 41MB is LESS than file size ({file_size_mb:.2f}MB) - This suggests incomplete unloading\")\n",
    "    elif 41 < file_size_mb * 1.5:\n",
    "        print(f\"⚠️  41MB is close to file size - May be memory-mapped or cached\")\n",
    "    elif 41 <= file_size_mb * 3:\n",
    "        print(f\"✅ 41MB is in expected range - Reasonable for ONNX Runtime\")\n",
    "    else:\n",
    "        print(f\"✅ 41MB is higher than expected - Good memory release\")\n",
    "\n",
    "# Run the smoke check\n",
    "smoke_check_model_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7a587-ed67-4cd2-9347-1bb625d9d8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c70ee-9da3-46dd-bfc6-7dee0529a0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad360568-713b-462d-ac5c-5c703d8c5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory mapped = on disk; \n",
    "# we are releasing he RAM, which is about 10% of our model file size, but not the disk.\n",
    "# To release memory mapped, need process separation, but the mapped memory can be resused withi the same process.\n",
    "def load_and_unload_multiple_models_v2(\n",
    "    model_paths: list,\n",
    "    model_name: str = \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    pause_duration: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Load 3 ONNX models and track memory release for each one individually.\n",
    "    Enhanced with file size comparison and memory mapping analysis.\n",
    "    \"\"\"\n",
    "    print(\"=== Multi-Model Loading and Unloading with Memory Tracking ===\")\n",
    "    print(f\"Loading {len(model_paths)} models...\")\n",
    "    print(f\"Pause duration between unloads: {pause_duration} seconds\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test texts for inference\n",
    "    test_texts = [\"I love this product!\", \"This is terrible!\", \"It's okay.\"]\n",
    "    \n",
    "    # Load tokenizer once (shared across models)\n",
    "    print(\"Loading shared tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Store loaded models\n",
    "    loaded_models = []\n",
    "    \n",
    "    # Step 1: Load all models and get file sizes\n",
    "    total_file_size = 0\n",
    "    for i, model_path in enumerate(model_paths, 1):\n",
    "        print(f\"\\n--- Loading Model {i}: {Path(model_path).name} ---\")\n",
    "        \n",
    "        if not Path(model_path).exists():\n",
    "            print(f\"❌ Model not found at {model_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Get file size\n",
    "        file_size_mb = Path(model_path).stat().st_size / 1024 / 1024\n",
    "        total_file_size += file_size_mb\n",
    "        print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        try:\n",
    "            # Load ONNX session\n",
    "            # This line does NOT load the model into RAM yet\n",
    "            # What actually happens:\n",
    "            #   1. ONNX Runtime opens the file\n",
    "            #   2. Maps the 475MB file to virtual memory address space\n",
    "            #   3. Reads the model metadata (structure, inputs, outputs)\n",
    "            #   4. Sets up the inference session\n",
    "            #   5. Allocates minimal memory for session management\n",
    "            #   6. Model weights stay on disk (memory-mapped)\n",
    "            # Memory impact after line executes:\n",
    "            #   - Virtual Memory (VMS): +475MB (mapped)\n",
    "            #   - Physical RAM (RSS): +~5-10MB (session overhead only)\n",
    "            #   - Disk I/O: Minimal (just metadata)\n",
    "            #   - Model weights: Still on disk, not in RAM\n",
    "            onnx_session = ort.InferenceSession(str(model_path))\n",
    "            \n",
    "            # Test inference to ensure model is loaded                       \n",
    "            tokenized = tokenizer(\n",
    "                test_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"np\"\n",
    "            )\n",
    "            \n",
    "            input_data = {\n",
    "                'input_ids': tokenized['input_ids'],\n",
    "                'attention_mask': tokenized['attention_mask']\n",
    "            }\n",
    "\n",
    "            # This line triggers actual model loading from disk\n",
    "            # What actually happens:\n",
    "            #   1. ONNX Runtime processes the input data\n",
    "            #   2. Accesses model weights for computation\n",
    "            #   3. OS triggers page faults for missing pages\n",
    "            #   4. Loads required model pages from disk into RAM\n",
    "            #   5. Performs inference computation\n",
    "            #   6. Returns results\n",
    "            # After this line:\n",
    "            #   - Virtual Memory (VMS): Still 475MB (mapped)\n",
    "            #   - Physical RAM (RSS): +~42MB (loaded pages)\n",
    "            #   - Disk I/O: Significant (loading model weights)\n",
    "            #   - Model weights: Partially in RAM (actively used portions)\n",
    "            outputs = onnx_session.run(None, input_data)\n",
    "            print(f\"✅ Model {i} loaded successfully\")\n",
    "            print(f\"   Output shape: {outputs[0].shape}\")\n",
    "            \n",
    "            # Store model info\n",
    "            loaded_models.append({\n",
    "                'index': i,\n",
    "                'path': model_path,\n",
    "                'name': Path(model_path).name,\n",
    "                'session': onnx_session,\n",
    "                'tokenizer': tokenizer,\n",
    "                'file_size_mb': file_size_mb\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load model {i}: {e}\")\n",
    "    \n",
    "    print(f\"\\n�� Loaded {len(loaded_models)} models successfully\")\n",
    "    print(f\"�� Total file size: {total_file_size:.2f} MB\")\n",
    "    \n",
    "    # Step 2: Unload each model individually and track memory\n",
    "    total_memory_freed = 0\n",
    "    memory_analysis = []\n",
    "    \n",
    "    for model_info in loaded_models:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"--- Unloading Model {model_info['index']}: {model_info['name']} ---\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Track memory release for this specific model\n",
    "        release_metrics = track_onnx_runtime_memory_release(\n",
    "            onnx_session=model_info['session'],\n",
    "            tokenizer=model_info['tokenizer'],\n",
    "            test_texts=test_texts\n",
    "        )\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        file_size = model_info['file_size_mb']\n",
    "        memory_freed = release_metrics['memory_freed_mb']\n",
    "        efficiency_ratio = memory_freed / file_size if file_size > 0 else 0\n",
    "        \n",
    "        # Display results for this model\n",
    "        print(f\"\\n📈 Memory Release Results for Model {model_info['index']}:\")\n",
    "        print(f\"   File size: {file_size:.2f} MB\")\n",
    "        print(f\"   Process memory freed: {memory_freed:.2f} MB\")\n",
    "        print(f\"   System memory freed: {release_metrics['system_memory_freed_mb']:.2f} MB\")\n",
    "        print(f\"   Memory freed percentage: {release_metrics['memory_freed_percent']:.2f}%\")\n",
    "        print(f\"   Unloading time: {release_metrics['unloading_time']:.4f} seconds\")\n",
    "        print(f\"   Efficiency ratio: {efficiency_ratio:.2f} ({efficiency_ratio*100:.1f}% of file size)\")\n",
    "        \n",
    "        # Memory mapping analysis\n",
    "        if efficiency_ratio < 0.5:\n",
    "            print(f\"   ⚠️  Low efficiency - likely memory-mapped or cached\")\n",
    "        elif efficiency_ratio < 1.0:\n",
    "            print(f\"   ⚠️  Partial release - memory-mapped model\")\n",
    "        elif efficiency_ratio < 2.0:\n",
    "            print(f\"   ✅ Good release - typical ONNX Runtime behavior\")\n",
    "        else:\n",
    "            print(f\"   ✅ Excellent release - better than expected\")\n",
    "        \n",
    "        # Accumulate totals\n",
    "        total_memory_freed += memory_freed\n",
    "        memory_analysis.append({\n",
    "            'model': model_info['name'],\n",
    "            'file_size': file_size,\n",
    "            'memory_freed': memory_freed,\n",
    "            'efficiency_ratio': efficiency_ratio\n",
    "        })\n",
    "        \n",
    "        # Pause before next unload\n",
    "        if model_info['index'] < len(loaded_models):\n",
    "            print(f\"\\n⏸️  Pausing for {pause_duration} seconds before next unload...\")\n",
    "            time.sleep(pause_duration)\n",
    "    \n",
    "    # Final summary with analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"📊 FINAL SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total models unloaded: {len(loaded_models)}\")\n",
    "    print(f\"Total file size: {total_file_size:.2f} MB\")\n",
    "    print(f\"Total process memory freed: {total_memory_freed:.2f} MB\")\n",
    "    print(f\"Average memory freed per model: {total_memory_freed/len(loaded_models):.2f} MB\")\n",
    "    print(f\"Overall efficiency: {total_memory_freed/total_file_size:.2f} ({total_memory_freed/total_file_size*100:.1f}% of total file size)\")\n",
    "    \n",
    "    # Memory mapping explanation\n",
    "    print(f\"\\n🔍 Memory Mapping Analysis:\")\n",
    "    print(f\"   Your models are likely memory-mapped by ONNX Runtime\")\n",
    "    print(f\"   This means the 475MB file stays in memory-mapped region\")\n",
    "    print(f\"   Only 42MB (9%) is freed because:\")\n",
    "    print(f\"     - File remains memory-mapped\")\n",
    "    print(f\"     - Runtime keeps caches for performance\")\n",
    "    print(f\"     - Memory pools are reused\")\n",
    "    print(f\"   For complete memory release, consider:\")\n",
    "    print(f\"     - Using separate processes\")\n",
    "    print(f\"     - Restarting the application\")\n",
    "    print(f\"     - Using memory-mapped file alternatives\")\n",
    "    \n",
    "    print(f\"Memory tracking completed!\")\n",
    "    \n",
    "    return memory_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa306bc-1205-4209-9833-a50a82b261f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Model Loading and Unloading with Memory Tracking ===\n",
      "Loading 3 models...\n",
      "Pause duration between unloads: 3 seconds\n",
      "============================================================\n",
      "Loading shared tokenizer...\n",
      "\n",
      "--- Loading Model 1: model.onnx ---\n",
      "   File size: 475.75 MB\n",
      "✅ Model 1 loaded successfully\n",
      "   Output shape: (3, 3)\n",
      "\n",
      "--- Loading Model 2: model.onnx ---\n",
      "   File size: 475.75 MB\n",
      "✅ Model 2 loaded successfully\n",
      "   Output shape: (3, 3)\n",
      "\n",
      "--- Loading Model 3: model.onnx ---\n",
      "   File size: 475.75 MB\n",
      "✅ Model 3 loaded successfully\n",
      "   Output shape: (3, 3)\n",
      "\n",
      "�� Loaded 3 models successfully\n",
      "�� Total file size: 1427.26 MB\n",
      "\n",
      "============================================================\n",
      "--- Unloading Model 1: model.onnx ---\n",
      "============================================================\n",
      "=== ONNX Runtime Memory Release Tracking ===\n",
      "Initial system memory: 3450.19 MB available\n",
      "Initial process memory: 2282.33 MB RSS\n",
      "Final inference completed\n",
      "Releasing ONNX Runtime memory...\n",
      "  - Session providers: ['CPUExecutionProvider']\n",
      "  - Session inputs: ['input_ids', 'attention_mask']\n",
      "  - Session outputs: ['logits']\n",
      "  - Destroying ONNX session...\n",
      "\n",
      "--- Memory Release Results ---\n",
      "Unloading time: 0.1227 seconds\n",
      "Process memory freed: -44.59 MB\n",
      "System memory freed: -51.72 MB\n",
      "Process memory freed: -45.47 MB\n",
      "Memory freed percentage: -0.27%\n",
      "⚠️  ONNX Runtime memory may not have been fully released\n",
      "   Note: ONNX Runtime often keeps memory allocated for performance\n",
      "   Consider using session destruction in a separate process for full release\n",
      "\n",
      "📈 Memory Release Results for Model 1:\n",
      "   File size: 475.75 MB\n",
      "   Process memory freed: -44.59 MB\n",
      "   System memory freed: -51.72 MB\n",
      "   Memory freed percentage: -0.27%\n",
      "   Unloading time: 0.1227 seconds\n",
      "   Efficiency ratio: -0.09 (-9.4% of file size)\n",
      "   ⚠️  Low efficiency - likely memory-mapped or cached\n",
      "\n",
      "⏸️  Pausing for 3 seconds before next unload...\n",
      "\n",
      "============================================================\n",
      "--- Unloading Model 2: model.onnx ---\n",
      "============================================================\n",
      "=== ONNX Runtime Memory Release Tracking ===\n",
      "Initial system memory: 3297.48 MB available\n",
      "Initial process memory: 2148.44 MB RSS\n",
      "Final inference completed\n",
      "Releasing ONNX Runtime memory...\n",
      "  - Session providers: ['CPUExecutionProvider']\n",
      "  - Session inputs: ['input_ids', 'attention_mask']\n",
      "  - Session outputs: ['logits']\n",
      "  - Destroying ONNX session...\n",
      "\n",
      "--- Memory Release Results ---\n",
      "Unloading time: 0.0753 seconds\n",
      "Process memory freed: -0.02 MB\n",
      "System memory freed: 33.39 MB\n",
      "Process memory freed: -66.44 MB\n",
      "Memory freed percentage: -0.00%\n",
      "✅ ONNX Runtime memory release successful\n",
      "\n",
      "📈 Memory Release Results for Model 2:\n",
      "   File size: 475.75 MB\n",
      "   Process memory freed: -0.02 MB\n",
      "   System memory freed: 33.39 MB\n",
      "   Memory freed percentage: -0.00%\n",
      "   Unloading time: 0.0753 seconds\n",
      "   Efficiency ratio: -0.00 (-0.0% of file size)\n",
      "   ⚠️  Low efficiency - likely memory-mapped or cached\n",
      "\n",
      "⏸️  Pausing for 3 seconds before next unload...\n",
      "\n",
      "============================================================\n",
      "--- Unloading Model 3: model.onnx ---\n",
      "============================================================\n",
      "=== ONNX Runtime Memory Release Tracking ===\n",
      "Initial system memory: 3331.56 MB available\n",
      "Initial process memory: 2201.95 MB RSS\n",
      "Final inference completed\n",
      "Releasing ONNX Runtime memory...\n",
      "  - Session providers: ['CPUExecutionProvider']\n",
      "  - Session inputs: ['input_ids', 'attention_mask']\n",
      "  - Session outputs: ['logits']\n",
      "  - Destroying ONNX session...\n",
      "\n",
      "--- Memory Release Results ---\n",
      "Unloading time: 0.0761 seconds\n",
      "Process memory freed: 0.00 MB\n",
      "System memory freed: 24.56 MB\n",
      "Process memory freed: -63.95 MB\n",
      "Memory freed percentage: 0.00%\n",
      "✅ ONNX Runtime memory release successful\n",
      "\n",
      "📈 Memory Release Results for Model 3:\n",
      "   File size: 475.75 MB\n",
      "   Process memory freed: 0.00 MB\n",
      "   System memory freed: 24.56 MB\n",
      "   Memory freed percentage: 0.00%\n",
      "   Unloading time: 0.0761 seconds\n",
      "   Efficiency ratio: 0.00 (0.0% of file size)\n",
      "   ⚠️  Low efficiency - likely memory-mapped or cached\n",
      "\n",
      "============================================================\n",
      "📊 FINAL SUMMARY\n",
      "============================================================\n",
      "Total models unloaded: 3\n",
      "Total file size: 1427.26 MB\n",
      "Total process memory freed: -44.61 MB\n",
      "Average memory freed per model: -14.87 MB\n",
      "Overall efficiency: -0.03 (-3.1% of total file size)\n",
      "\n",
      "🔍 Memory Mapping Analysis:\n",
      "   Your models are likely memory-mapped by ONNX Runtime\n",
      "   This means the 475MB file stays in memory-mapped region\n",
      "   Only 42MB (9%) is freed because:\n",
      "     - File remains memory-mapped\n",
      "     - Runtime keeps caches for performance\n",
      "     - Memory pools are reused\n",
      "   For complete memory release, consider:\n",
      "     - Using separate processes\n",
      "     - Restarting the application\n",
      "     - Using memory-mapped file alternatives\n",
      "Memory tracking completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model': 'model.onnx',\n",
       "  'file_size': 475.75407218933105,\n",
       "  'memory_freed': -44.59375,\n",
       "  'efficiency_ratio': -0.09373277625305007},\n",
       " {'model': 'model.onnx',\n",
       "  'file_size': 475.75407218933105,\n",
       "  'memory_freed': -0.015625,\n",
       "  'efficiency_ratio': -3.284259854696919e-05},\n",
       " {'model': 'model.onnx',\n",
       "  'file_size': 475.75407218933105,\n",
       "  'memory_freed': 0.0,\n",
       "  'efficiency_ratio': 0.0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths = [\n",
    "        \"/Users/chang/.cache/huggingface/hub/onnx_models/cardiffnlp_twitter-roberta-base-sentiment-latest/model.onnx\",\n",
    "        \"/Users/chang/.cache/huggingface/hub/onnx_models/cardiffnlp_twitter-roberta-base-sentiment-latest/model.onnx\",  # Same model for demo\n",
    "        \"/Users/chang/.cache/huggingface/hub/onnx_models/cardiffnlp_twitter-roberta-base-sentiment-latest/model.onnx\"   # Same model for demo\n",
    "    ]\n",
    "    \n",
    "# You can replace with different models:\n",
    "# model_paths = [\n",
    "#     \"/path/to/model1.onnx\",\n",
    "#     \"/path/to/model2.onnx\", \n",
    "#     \"/path/to/model3.onnx\"\n",
    "# ]\n",
    "\n",
    "#load_and_unload_multiple_models(\n",
    "load_and_unload_multiple_models_v2(\n",
    "    model_paths=model_paths,\n",
    "    model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    pause_duration=3  # 3 seconds pause between unloads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d0a7a-fa1d-4d91-80f3-efb0823b80a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coach Kata (uv)",
   "language": "python",
   "name": "coachkata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
